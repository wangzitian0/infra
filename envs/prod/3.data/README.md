# 3.data (Data Layer / Layer 3) - Production Environment

> **环境**: Production
> **定位**: 模块维护文档（面向基础设施运维者）

**Scope**:
- **Relational**: Business PostgreSQL (Application data)
- **Cache**: Redis
- **Multi-model**: ArangoDB (Document, Graph, Key-Value)
- **OLAP**: ClickHouse
- **Namespace**: `data-<env>` (e.g. `data-prod`, `data-prod`)

## Architecture

This layer provides stateful services for **Business Applications** (L4).

*Note: Platform DB (for Vault/Casdoor) is in L1 (`1.bootstrap/5.platform_pg.tf`).*

### Password Flow (Vault-first Pattern - Issue #349)

```mermaid
graph LR
    subgraph "L2 Platform"
        VM[vault_mount.kv]
        VD[vault_mount.database]
    end
    subgraph "L3 Data"
        K8S[K8s Secret] --> |recovery| LOCAL[local.*_password]
        RND[random_password] --> |first deploy| LOCAL
        LOCAL --> KV[Vault KV]
        LOCAL --> HELM[DB Helm Charts]
        LOCAL --> DB_CFG[database_secret_backend]
    end
    VM --> KV
    VD --> DB_CFG
    DB_CFG --> |dynamic creds| L4[L4 Apps]
    KV --> |static creds| L4
```

**L3 owns password generation** - L2 only creates Vault mounts.

**State Recovery Pattern**: When Terraform state is lost but resources exist:
1. `data.external.*_password` reads existing password from Vault (SSOT)
2. `local.*_password` selects existing or generates new
3. Helm chart uses same password → no restart needed
4. Vault secret has `ignore_changes` → no credential overwrite


### Components

| File | Component | Purpose |
|------|-----------|---------|
| `1.postgres.tf` | PostgreSQL | Business database, creds from Vault KV |
| `2.redis.tf` | Redis | Cache and session storage |
| `3.clickhouse.tf` | ClickHouse | OLAP analytics database (Keeper disabled for single-node) |
| `4.arangodb.tf` | ArangoDB | Multi-model database (document/graph/KV) |

### Deployment Order

1. **L1** (Bootstrap): k3s, Platform PostgreSQL
2. **L2** (Platform): Vault, vault_mount, password generation, database engine config
3. **L3** (Data): Read password from Vault, deploy PostgreSQL Helm
4. **L4** (Apps): Get dynamic credentials via Vault Agent

### Credentials

| Service | Vault Path | Type | Dynamic |
|---------|------------|------|---------|
| PostgreSQL root | `secret/data/postgres` | Static (L3 generated) | ✅ |
| PostgreSQL app users | `database/creds/postgres-*` | Dynamic | ✅ |
| Redis | `secret/data/redis` | Static (L3 generated) | ✅ |
| Redis app users | `database/creds/redis-*` | Dynamic | ✅ |
| ClickHouse | `secret/data/clickhouse` | Static (L3 generated) | ⏳ |
| ArangoDB | `secret/data/arangodb` | Static (L3 generated) | ⏳ |

## Design Decisions

### Deployment & Configuration

**Terragrunt Integration**:
- **Backend/Providers**: Auto-generated by root `terragrunt.hcl` (gitignored)
- **State key**: `k3s/data-prod.tfstate`
- **Environment**: Automatically detected from directory path

```bash
# Standalone usage
cd envs/prod/3.data
export R2_BUCKET=<bucket> R2_ACCOUNT_ID=<account-id>
terragrunt init
terragrunt apply
```

### Namespace Ownership

The `data-prod` namespace is **owned by L3** (`envs/prod/3.data/1.postgres.tf`). This follows the pattern:
- L1 owns `kube-system`, `platform` (namespace created in L1)
- L2 operates within `platform` (namespace passed from L1)
- **L3 owns `data-prod`** (namespace created in L3 prod)
- L4 operates within `apps-prod` (and may create app-specific namespaces)

### Dual-SSOT Password Rationale

Password is stored in **both Vault KV and Helm values**:

| Location | Purpose | Justification |
|----------|---------|---------------|
| Vault KV (`secret/data/postgres`) | Dynamic credential generation | Vault Database Engine uses this to create short-lived app users |
| Helm values (via random_password) | Initial PostgreSQL deployment | PostgreSQL needs password at install time |

**Why not Vault Agent Injector for PostgreSQL itself?**
- PostgreSQL is the **database itself**, not an app consuming it
- The root password must exist at PostgreSQL startup
- Vault Agent pattern is for **consumers** (L4 apps), not the database server

## Disaster Recovery

### Backup Strategy

```bash
# Backup L3 PostgreSQL (run on VPS)
NS="data-prod" # or data-prod
kubectl exec -n "$NS" postgresql-0 -- pg_dump -U postgres app > l3_backup.sql

# Restore
kubectl exec -n "$NS" postgresql-0 -- psql -U postgres -d app < l3_backup.sql
```

### Recovery Steps

1. **Terraform State Lost** (Issue #349 pattern):
   - Re-run L3 apply → reads existing passwords from Vault (SSOT)
   - No database restart needed (same credentials)
   - Vault secrets protected by `ignore_changes`
2. **Data Loss**: Restore from pg_dump backup
3. **Full Recreation**: Delete `data-<env>` namespace, re-apply L3

## Health Checks & Validation

All L3 data services follow the health check matrix from [ops.pipeline.md](../docs/ssot/ops.pipeline.md#8-健康检查分层):

### Atlantis (In-Cluster Providers)

When `TF_VAR_kubeconfig_path` is empty (Atlantis in-cluster), the `kubectl` provider must set `load_config_file=false` to avoid defaulting to `http://localhost` and failing CRD applies (e.g., `kubectl_manifest`).

### Terraform-native Validation

Each service has **lifecycle preconditions** to ensure Vault KV availability:

```hcl
lifecycle {
  prevent_destroy = true  # Prevent accidental data loss
  
  precondition {
    condition     = can(data.vault_kv_secret_v2.{service}.data["password"]) && 
                    length(data.vault_kv_secret_v2.{service}.data["password"]) >= 16
    error_message = "{Service} password must be available in Vault KV and at least 16 characters."
  }
}
```

**Coverage**:
- ✅ **PostgreSQL**: Password validation (16+ chars)
- ✅ **Redis**: Password validation (16+ chars)  
- ✅ **ClickHouse**: Password validation (16+ chars)
- ✅ **ArangoDB**: Password validation (16+ chars) + JWT secret precondition

### Helm Chart Configuration

- **Timeout**: 300s (standardized across all releases)
- **Wait**: `wait = true` ensures readiness before completion
- **Lifecycle**: `prevent_destroy = true` on all database releases
- **Storage**: follow [ops.storage.md](../docs/ssot/ops.storage.md) (`local-path-retain`, reclaim policy, /data conventions)

**Note**: initContainer and other Pod-level health checks are configured via Helm chart defaults, not in Terraform values per SSOT guidelines.
Scalability for MVP:
- **Redis**: Master-only (no replicas)
- **ClickHouse**: Single shard, no ZooKeeper
- **ArangoDB**: Single mode (not Cluster)

Scale-out / multi-replica migration notes are tracked in:
- [docs/project/README.md](../docs/project/README.md) (execution plan / backlog)
- [db.overview.md](../docs/ssot/db.overview.md) (database capability SSOT)

---
*Last updated: 2025-12-23 (Vault-first password pattern - Issue #349)*
