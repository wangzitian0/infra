#!/usr/bin/env python3
"""
Aggregate .env.ci keys per workspace and check/update the workspace schema.

Workspace layout (at repo root):
workspace_<name>/workspace.toml
workspace_<name>/.env.ci      # aggregated schema (committed)
workspace_<name>/.env.test    # optional concrete envs
workspace_<name>/.env.prod
"""

import argparse
import re
import sys
from pathlib import Path
from typing import Dict, Iterable, Set

try:
    import tomllib  # py3.11+
except ModuleNotFoundError:  # pragma: no cover
    import tomli as tomllib  # type: ignore

try:
    from dotenv import dotenv_values  # python-dotenv
except ImportError:
    dotenv_values = None

ENV_PATTERN = re.compile(r"^\s*(?:export\s+)?([A-Za-z_][A-Za-z0-9_]*)\s*=(.*)$")


def parse_env_file(path: Path):
    if dotenv_values is not None:
        data = dotenv_values(path)
        return {k: f"{k}={v}" for k, v in data.items() if k}
    # fallback to regex parsing
    keys: Dict[str, str] = {}
    for line in path.read_text(encoding="utf-8").splitlines():
        if not line.strip() or line.lstrip().startswith("#"):
            continue
        m = ENV_PATTERN.match(line)
        if m:
            key, val = m.group(1), m.group(2).strip()
            keys.setdefault(key, f"{key}={val}")
    return keys


def collect_paths(repo_root: Path) -> Iterable[Path]:
    return [p for p in repo_root.rglob(".env.ci") if p.is_file()]


def load_workspace(workspace_dir: Path) -> Dict[str, Path]:
    cfg_path = workspace_dir / "workspace.toml"
    data = tomllib.loads(cfg_path.read_text(encoding="utf-8"))
    repos = data.get("repos") or []
    resolved: Dict[str, Path] = {}
    for repo in repos:
        name = repo.get("name")
        rel = repo.get("path") or name
        if not name or not rel:
            raise ValueError("workspace.toml repo entry requires 'name' and 'path'")
        rel_path = Path(rel).expanduser()
        resolved_path = rel_path if rel_path.is_absolute() else (workspace_dir / rel_path)
        resolved[name] = resolved_path.resolve()
    return resolved


def aggregate(workspace_dir: Path):
    repo_paths = load_workspace(workspace_dir)
    mapping: Dict[str, Set[Path]] = {}
    lines: Dict[str, str] = {}
    for repo_name, repo_root in repo_paths.items():
        if not repo_root.exists():
            sys.stderr.write(f"[warn] repo path missing: {repo_root} ({repo_name})\n")
            continue
        for env_file in collect_paths(repo_root):
            try:
                rel_path = env_file.relative_to(workspace_dir)
            except ValueError:
                # store path relative to repo root with repo name prefix
                rel_path = Path(f"{repo_name}/{env_file.relative_to(repo_root)}")
            parsed = parse_env_file(env_file)
            for key, line in parsed.items():
                if key in lines and lines[key] != line:
                    sys.stderr.write(f"[warn] duplicate key with differing value: {key} ({rel_path})\n")
                lines.setdefault(key, line)
                mapping.setdefault(key, set()).add(rel_path)
    return mapping, lines


def render(mapping: Dict[str, Set[Path]], lines_map: Dict[str, str]) -> str:
    lines = ["# Aggregated schema, generated by tool_env_vars/collect_env_ci.py"]
    for key in sorted(mapping):
        paths = ", ".join(str(p) for p in sorted(mapping[key]))
        content = lines_map.get(key, key + "=")
        lines.append(f"{content}  # {paths}")
    lines.append("")
    return "\n".join(lines)


def check_uniqueness(mapping: Dict[str, Set[Path]]) -> bool:
    duplicates = {k: v for k, v in mapping.items() if len(v) > 1}
    if not duplicates:
        return True
    sys.stderr.write("[error] duplicate keys across .env.ci within workspace:\n")
    for key, paths in duplicates.items():
        joined = ", ".join(str(p) for p in sorted(paths))
        sys.stderr.write(f"  {key}: {joined}\n")
    return False


def main() -> int:
    parser = argparse.ArgumentParser(description="Aggregate .env.ci keys per workspace.")
    parser.add_argument("workspace", help="Workspace name (e.g., truealpha)")
    parser.add_argument("--root", type=Path, default=Path(__file__).resolve().parent.parent, help="Repo root")
    parser.add_argument("--check", action="store_true", help="Check drift against schema")
    parser.add_argument("--update", action="store_true", help="Update workspace .env.ci")
    args = parser.parse_args()

    candidate = args.root / f"workspace_{args.workspace}"
    workspace_dir = candidate if candidate.exists() else (args.root / args.workspace)
    workspace_dir = workspace_dir.resolve()
    if not workspace_dir.exists():
        sys.stderr.write(f"[error] workspace dir missing: {workspace_dir}\n")
        return 2

    mapping, lines_map = aggregate(workspace_dir)
    if not check_uniqueness(mapping):
        return 1

    content = render(mapping, lines_map)
    schema_path = workspace_dir / ".env.ci"

    if args.check:
        if not schema_path.exists():
            sys.stderr.write(f"[check] missing schema file: {schema_path}\n")
            return 1
        if schema_path.read_text(encoding="utf-8") != content:
            sys.stderr.write("[check] schema drift detected; run with --update\n")
            return 1
        return 0

    if args.update:
        schema_path.write_text(content, encoding="utf-8")
        print(f"[update] wrote {schema_path} ({len(mapping)} keys)")
        return 0

    sys.stdout.write(content)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
