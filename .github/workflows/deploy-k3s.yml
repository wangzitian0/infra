name: Deploy k3s to VPS

on:
  push:
    branches: [main]
    paths:
      - "1.bootstrap/**"
      - ".github/workflows/deploy-k3s.yml"
      - ".github/actions/**"
  workflow_dispatch: {}

env:
  TF_IN_AUTOMATION: "true"
  TF_INPUT: "false"

jobs:
  apply:
    runs-on: ubuntu-latest

    permissions:
      contents: read

    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Setup Terraform Environment (L1)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "1.bootstrap"
          terraform_version: 1.6.6
          tf_state_key: "k3s/terraform.tfstate"
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          k3s_cluster_name: ${{ secrets.K3S_CLUSTER_NAME }}
          k3s_api_endpoint: ${{ secrets.K3S_API_ENDPOINT }}
          k3s_channel: ${{ secrets.K3S_CHANNEL }}
          k3s_version: ${{ secrets.K3S_VERSION }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          internal_domain: ${{ secrets.INTERNAL_DOMAIN }}
          internal_zone_id: ${{ secrets.INTERNAL_ZONE_ID }}
          github_app_id: ${{ secrets.ATLANTIS_GH_APP_ID }}
          github_app_key: ${{ secrets.ATLANTIS_GH_APP_KEY }}
          atlantis_web_password: ${{ secrets.ATLANTIS_WEB_PASSWORD }}
          # OAuth2-Proxy (GitHub OAuth for Dashboard protection)
          github_oauth_client_id: ${{ secrets.GH_OAUTH_CLIENT_ID }}
          github_oauth_client_secret: ${{ secrets.GH_OAUTH_CLIENT_SECRET }}
          # L3: Vault Access
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}

      # Pre-flight check: Validate Helm URLs and other common issues
      - name: Pre-flight Check
        run: |
          chmod +x 0.tools/preflight-check.sh
          0.tools/preflight-check.sh

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: v1.28.4

      # Import existing k8s resources that Terraform wants to manage
      # This prevents "already exists" errors when adopting existing resources
      - name: Import Existing Resources
        working-directory: 1.bootstrap
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          # Import local-path-config ConfigMap if not in state
          # Note: Using flat resource path after directory restructure
          if ! terraform state show kubernetes_config_map_v1.local_path_config 2>/dev/null; then
            echo "Importing local-path-config ConfigMap..."
            terraform import kubernetes_config_map_v1.local_path_config kube-system/local-path-config || true
          fi
          
          # If Atlantis already exists in cluster but was lost from state, import it.
          # Never remove state or delete in-cluster resources automatically from CI.
          if terraform state show helm_release.atlantis 2>/dev/null; then
            echo "Atlantis helm_release already tracked in Terraform state."
          else
            echo "Attempting to import existing Atlantis helm release (if present)..."
            terraform import helm_release.atlantis bootstrap/atlantis || true
          fi
          
          # Force clean Helm secrets to prevent "cannot re-use a name" error
          # This handles the case where Terraform state is clean but Cluster has lingering Helm release
          echo "Force cleaning lingering Helm secrets..."
          echo "Force cleaning lingering Helm secrets..."
          # Use pattern matching instead of labels to avoid accidents and ensure we hit the right target
          kubectl -n platform get secrets -o name | grep "sh.helm.release.v1.atlantis" | xargs -r kubectl -n platform delete || true
          kubectl delete deployment -n platform atlantis || true
          kubectl delete svc -n platform atlantis || true
          kubectl delete statefulset -n platform atlantis || true

      # =========================================================
      # 1. Bootstrap Layer (L1): K3s, Atlantis, Cert-Manager
      # =========================================================
      - name: Apply Infrastructure (L1)
        working-directory: 1.bootstrap
        env:
          TF_LOG: INFO
        run: |
          terraform apply -auto-approve || {
            echo "::error::L1 Apply failed. Checking pod status..."
            kubectl get pods -A -o wide 2>/dev/null || true
            kubectl describe pods -n platform 2>/dev/null | tail -50 || true
            exit 1
          }

      - name: Verify L1 (Smoke Test)
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          kubectl version --client
          kubectl get nodes -o wide

      # =========================================================
      # 2. Platform Layer (L2): Vault, Observability, Ingress
      # =========================================================
      - name: Setup Terraform Environment (L2)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "2.platform"
          terraform_version: 1.6.6
          tf_state_key: "k3s/platform.tfstate"
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          internal_domain: ${{ secrets.INTERNAL_DOMAIN }}
          internal_zone_id: ${{ secrets.INTERNAL_ZONE_ID }}
          github_oauth_client_id: ${{ secrets.GH_OAUTH_CLIENT_ID }}
          github_oauth_client_secret: ${{ secrets.GH_OAUTH_CLIENT_SECRET }}
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}

      - name: Apply Infrastructure (L2 - Platform)
        working-directory: 2.platform
        env:
          TF_LOG: INFO
        run: |
          # Ensure platform workspace is default
          terraform workspace select default || terraform workspace new default
          terraform apply -auto-approve || {
             echo "::error::L2 Platform Apply failed."
             exit 1
          }

      - name: Verify L2 (Vault Status)
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          echo "Checking Vault Status..."
          kubectl -n platform get pods -l app.kubernetes.io/name=vault
          # Simple check if Vault is running and accessible inside cluster
          # (Detailed check requires init, which is handled by raft join in L2 setup)

      # =========================================================
      # 3. Data Layer (L3): Databases
      # =========================================================
      - name: Setup Terraform Environment (L3)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "3.data"
          terraform_version: 1.6.6
          tf_state_key: "k3s/data-prod.tfstate" # Hardcoded for main branch -> prod
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}

      - name: Pre-flight Image Check (L3)
        if: github.event_name == 'push'
        run: |
          python3 tools/check_images.py \
            "bitnamilegacy/postgresql:17.2.0-debian-12-r8" \
            "bitnamilegacy/redis:7.4.2-debian-12-r0" \
            "bitnamilegacy/clickhouse:25.7.5-debian-12-r0" \
            "bitnamilegacy/clickhouse-keeper:25.7.5-debian-12-r0" \
            "arangodb/arangodb:3.11.8"

      - name: Apply Infrastructure (L3 - Data)
        if: github.event_name == 'push'
        working-directory: 3.data
        env:
          TF_LOG: INFO
        run: |
          # Use workspace based on branch mapping (main -> prod)
          terraform workspace select prod || terraform workspace new prod
          terraform apply -auto-approve || {
             echo "::error::L3 Data Apply failed."
             exit 1
          }

      - name: Verify Data Services Health (L3)
        if: github.event_name == 'push'
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          NS="data-prod" # Mapping main branch to prod env
          TIMEOUT=180
          
          echo "=== 1. Waiting for Pod Readiness (K8s Layer) ==="
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=postgresql --timeout=${TIMEOUT}s || echo "⚠️ PostgreSQL wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=redis --timeout=${TIMEOUT}s || echo "⚠️ Redis wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=clickhouse --timeout=${TIMEOUT}s || echo "⚠️ ClickHouse wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l arango_deployment=arangodb --timeout=${TIMEOUT}s || echo "⚠️ ArangoDB wait timeout"
          
          echo ""
          
          echo "=== 2. Application Layer Connectivity Checks ==="
          
          # Retrieve Secrets for Health Checks
          REDIS_PASS=$(kubectl get secret -n $NS redis -o jsonpath="{.data.redis-password}" | base64 -d)
          CH_PASS=$(kubectl get secret -n $NS clickhouse -o jsonpath="{.data.admin-password}" | base64 -d)

          echo "Checking PostgreSQL..."
          kubectl exec -n $NS postgresql-0 -- pg_isready -U postgres && echo "✅ PostgreSQL: Ready" || echo "❌ PostgreSQL: Not Ready"
          
          echo "Checking Redis..."
          kubectl exec -n $NS redis-master-0 -- redis-cli -a "$REDIS_PASS" ping && echo "✅ Redis: PONG" || echo "❌ Redis: Not Ready"
          
          echo "Checking ClickHouse..."
          kubectl exec -n $NS clickhouse-shard0-0 -- clickhouse-client --user default --password "$CH_PASS" --query "SELECT 1" && echo "✅ ClickHouse: Ready" || echo "❌ ClickHouse: Not Ready"
          
          echo "Checking Keeper..."
          kubectl exec -n $NS clickhouse-keeper-0 -- bash -c 'echo ruok | nc localhost 9181' && echo "✅ Keeper: imok" || echo "❌ Keeper: Not Ready"
          
          echo "Checking ArangoDB..."
          kubectl exec -n $NS -l arango_deployment=arangodb -- curl -sf http://localhost:8529/_api/version && echo "✅ ArangoDB: Ready" || echo "❌ ArangoDB: Not Ready"
          
          echo ""
          echo "=== Summary ==="
          kubectl get pods -n $NS -o wide

      # =========================================================
      # 4. App Layer (L4): Applications
      # =========================================================
      - name: Setup Terraform Environment (L4)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "4.apps"
          terraform_version: 1.6.6
          tf_state_key: "k3s/apps-prod.tfstate" # Hardcoded for main branch -> prod
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}

      - name: Apply Infrastructure (L4 - Apps)
        if: github.event_name == 'push'
        working-directory: 4.apps
        env:
          TF_LOG: INFO
        run: |
          terraform workspace select prod || terraform workspace new prod
          terraform apply -auto-approve || {
             echo "::error::L4 Apps Apply failed."
             exit 1
          }

      - name: Verify L4 (Endpoint Check)
        if: github.event_name == 'push'
        run: |
          echo "Verifying L4 Applications..."
          # Placeholder for specific app checks (e.g., curl endpoints)
          kubectl get pods -n apps-prod || true

      - name: Upload kubeconfig
        uses: actions/upload-artifact@v4
        with:
          name: kubeconfig
          path: ${{ env.KUBECONFIG_PATH }}
