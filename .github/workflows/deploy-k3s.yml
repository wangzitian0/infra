name: Deploy k3s to VPS

on:
  push:
    branches: [main]
    paths:
      # Trigger on infrastructure code changes (all layers)
      - "1.bootstrap/**"
      - "2.platform/**"
      - "3.data/**"
      - "4.apps/**"
      - ".github/workflows/deploy-k3s.yml"
      - ".github/actions/**"
      # Excluded: docs/** and *.md (no infra changes)
  workflow_dispatch: {}

env:
  TF_IN_AUTOMATION: "true"
  TF_INPUT: "false"

jobs:
  apply:
    runs-on: ubuntu-latest

    permissions:
      contents: read

    env:
      # Common secrets used across multiple steps
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      R2_BUCKET: ${{ secrets.R2_BUCKET }}
      R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
      VPS_HOST: ${{ secrets.VPS_HOST }}
      VPS_SSH_KEY: ${{ secrets.VPS_SSH_KEY }}
      VPS_USER: ${{ secrets.VPS_USER }}
      VPS_SSH_PORT: ${{ secrets.VPS_SSH_PORT }}
      BASE_DOMAIN: ${{ secrets.BASE_DOMAIN }}
      INTERNAL_DOMAIN: ${{ secrets.INTERNAL_DOMAIN }}
      VAULT_ROOT_TOKEN: ${{ secrets.VAULT_ROOT_TOKEN }}
      VAULT_POSTGRES_PASSWORD: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
      ATLANTIS_WEBHOOK_SECRET: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
      CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
      CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}


    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true

      # =========================================================
      # PRE-FLIGHT PHASE 0: Validate Critical Inputs (Shift-Left)
      # =========================================================
      - name: Pre-flight (0-Inputs)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          VPS_HOST: ${{ secrets.VPS_HOST }}
          VPS_SSH_KEY: ${{ secrets.VPS_SSH_KEY }}
          VAULT_POSTGRES_PASSWORD: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          ATLANTIS_WEBHOOK_SECRET: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ZONE_ID: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          BASE_DOMAIN: ${{ secrets.BASE_DOMAIN }}
          VAULT_ROOT_TOKEN: ${{ secrets.VAULT_ROOT_TOKEN }}

        run: |
          echo "=== Pre-flight Phase 0: Input Validation ==="
          FAILED=0
          
          # Critical secrets that MUST exist
          REQUIRED="AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY R2_BUCKET R2_ACCOUNT_ID \
                    VPS_HOST VPS_SSH_KEY VAULT_POSTGRES_PASSWORD ATLANTIS_WEBHOOK_SECRET \
                    CLOUDFLARE_API_TOKEN CLOUDFLARE_ZONE_ID BASE_DOMAIN VAULT_ROOT_TOKEN"
          
          for VAR in $REQUIRED; do
            if [ -z "${!VAR}" ]; then
              echo "❌ Missing required secret: $VAR"
              FAILED=1
            else
              echo "✅ $VAR: present"
            fi
          done
          
          if [ $FAILED -eq 1 ]; then
            echo ""
            echo "::error::Pre-flight failed: Missing required secrets. Check GitHub Secrets configuration."
            exit 1
          fi
          
          echo ""
          echo "✅ All required secrets validated"

      - name: Setup Terraform Environment (L1)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "1.bootstrap"
          terraform_version: 1.6.6
          tf_state_key: "k3s/terraform.tfstate"
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          k3s_cluster_name: ${{ secrets.K3S_CLUSTER_NAME }}
          k3s_api_endpoint: ${{ secrets.K3S_API_ENDPOINT }}
          k3s_channel: ${{ secrets.K3S_CHANNEL }}
          k3s_version: ${{ secrets.K3S_VERSION }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          internal_domain: ${{ secrets.INTERNAL_DOMAIN }}
          internal_zone_id: ${{ secrets.INTERNAL_ZONE_ID }}
          github_app_id: ${{ secrets.ATLANTIS_GH_APP_ID }}
          github_app_key: ${{ secrets.ATLANTIS_GH_APP_KEY }}
          atlantis_web_password: ${{ secrets.ATLANTIS_WEB_PASSWORD }}
          # OAuth2-Proxy (GitHub OAuth for Dashboard protection)
          github_oauth_client_id: ${{ secrets.GH_OAUTH_CLIENT_ID }}
          github_oauth_client_secret: ${{ secrets.GH_OAUTH_CLIENT_SECRET }}
          # L3: Vault Access
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}

      # Pre-flight check: Validate Helm URLs and other common issues
      - name: Pre-flight Check
        run: |
          chmod +x 0.tools/preflight-check.sh
          0.tools/preflight-check.sh

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: v1.28.4

      # Import existing k8s resources that Terraform wants to manage
      # This prevents "already exists" errors when adopting existing resources
      - name: Import Existing Resources
        working-directory: 1.bootstrap
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          # Import local-path-config ConfigMap if not in state
          # Note: Using flat resource path after directory restructure
          if ! terraform state show kubernetes_config_map_v1.local_path_config 2>/dev/null; then
            echo "Importing local-path-config ConfigMap..."
            terraform import kubernetes_config_map_v1.local_path_config kube-system/local-path-config || true
          fi
          
          # If Atlantis already exists in cluster but was lost from state, import it.
          # Never remove state or delete in-cluster resources automatically from CI.
          if terraform state show helm_release.atlantis 2>/dev/null; then
            echo "Atlantis helm_release already tracked in Terraform state."
          else
            echo "Attempting to import existing Atlantis helm release (if present)..."
            terraform import helm_release.atlantis bootstrap/atlantis || true
          fi
          
      # =========================================================
      # PRE-FLIGHT PHASE 3: State Consistency (Shift-Left)
      # Prevents "cannot re-use a name that is still in use" errors
      # =========================================================
      - name: Pre-flight (3-State)
        working-directory: 1.bootstrap
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          echo "=== Pre-flight Phase 3: State Consistency ==="
          
          # Check if Atlantis exists in cluster but not in TF state
          TF_HAS_ATLANTIS=$(terraform state show helm_release.atlantis 2>/dev/null && echo "yes" || echo "no")
          HELM_HAS_ATLANTIS=$(helm status atlantis -n platform 2>/dev/null && echo "yes" || echo "no")
          
          echo "TF State has Atlantis: $TF_HAS_ATLANTIS"
          echo "Helm has Atlantis: $HELM_HAS_ATLANTIS"
          
          if [ "$HELM_HAS_ATLANTIS" = "yes" ] && [ "$TF_HAS_ATLANTIS" = "no" ]; then
            echo "⚠️ State mismatch detected: Helm has Atlantis but TF doesn't track it"
            echo "Cleaning up to allow fresh install..."
            kubectl -n platform get secrets -o name | grep "sh.helm.release.v1.atlantis" | xargs -r kubectl -n platform delete || true
            kubectl delete deployment -n platform atlantis 2>/dev/null || true
            kubectl delete svc -n platform atlantis 2>/dev/null || true
            kubectl delete statefulset -n platform atlantis 2>/dev/null || true
            echo "✅ Cleanup complete"
          elif [ "$HELM_HAS_ATLANTIS" = "no" ] && [ "$TF_HAS_ATLANTIS" = "yes" ]; then
            echo "⚠️ State mismatch detected: TF knows Atlantis but Helm doesn't"
            echo "Removing stale TF state..."
            terraform state rm helm_release.atlantis || true
            echo "✅ State cleanup complete"
          else
            echo "✅ State is consistent"
          fi

      # =========================================================
      # 1. Bootstrap Layer (L1): K3s, Atlantis, Cert-Manager
      # =========================================================
      - name: Apply Infrastructure (L1)
        working-directory: 1.bootstrap
        env:
          TF_LOG: INFO
        run: |
          terraform apply -auto-approve || {
            echo "::error::L1 Apply failed. Checking pod status..."
            kubectl get pods -A -o wide 2>/dev/null || true
            kubectl describe pods -n platform 2>/dev/null | tail -50 || true
            exit 1
          }

      - name: Verify L1 (Smoke Test)
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          kubectl version --client
          kubectl get nodes -o wide

      # =========================================================
      # 2. Platform Layer (L2): Vault, Observability, Ingress
      # =========================================================
      - name: Setup Terraform Environment (L2)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "2.platform"
          terraform_version: 1.6.6
          tf_state_key: "k3s/platform.tfstate"
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          internal_domain: ${{ secrets.INTERNAL_DOMAIN }}
          internal_zone_id: ${{ secrets.INTERNAL_ZONE_ID }}
          github_oauth_client_id: ${{ secrets.GH_OAUTH_CLIENT_ID }}
          github_oauth_client_secret: ${{ secrets.GH_OAUTH_CLIENT_SECRET }}
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}
          vault_address: https://secrets.${{ secrets.INTERNAL_DOMAIN }}

      # =========================================================
      # PRE-FLIGHT PHASE 2: Verify External Dependencies (Shift-Left)
      # =========================================================
      - name: Pre-flight (2-Dependencies)
        env:
          INTERNAL_DOMAIN: ${{ secrets.INTERNAL_DOMAIN }}
          VAULT_ROOT_TOKEN: ${{ secrets.VAULT_ROOT_TOKEN }}
        run: |
          echo "=== Pre-flight Phase 2: Dependency Check ==="
          
          # Vault external URL reachable?
          VAULT_URL="https://secrets.${INTERNAL_DOMAIN}"
          echo "Checking Vault at ${VAULT_URL}..."
          
          HEALTH_RESPONSE=$(curl -sf --connect-timeout 10 "${VAULT_URL}/v1/sys/health" 2>&1 || echo "FAIL")
          
          if [ "$HEALTH_RESPONSE" = "FAIL" ]; then
            echo "❌ Vault is not reachable at ${VAULT_URL}"
            echo "::error::Pre-flight failed: Vault unreachable. Check if Vault is running."
            exit 1
          fi
          
          # Check if response is JSON (not HTML)
          if echo "$HEALTH_RESPONSE" | grep -q "^<"; then
            echo "❌ Vault returned HTML (may be SSO gate or error page)"
            echo "   Response preview: $(echo "$HEALTH_RESPONSE" | head -c 200)"
            echo "::error::Pre-flight failed: Vault returned HTML instead of JSON"
            exit 1
          fi
          
          echo "✅ Vault is reachable at ${VAULT_URL}"
          
          # Vault token valid?
          echo "Checking Vault token validity..."
          TOKEN_RESPONSE=$(curl -s --connect-timeout 10 \
            -H "X-Vault-Token: ${VAULT_ROOT_TOKEN}" \
            "${VAULT_URL}/v1/auth/token/lookup-self" 2>&1)
          
          # Check if response is HTML (SSO page)
          if echo "$TOKEN_RESPONSE" | grep -q "^<"; then
            echo "❌ Vault token check returned HTML (token may be blocked by SSO)"
            echo "   Response preview: $(echo "$TOKEN_RESPONSE" | head -c 200)"
            echo "::error::Pre-flight failed: Vault token blocked by SSO gate"
            exit 1
          fi
          
          # Check if token lookup succeeded
          if echo "$TOKEN_RESPONSE" | grep -q "errors"; then
            echo "❌ Vault token is invalid or expired"
            echo "   Error: $(echo "$TOKEN_RESPONSE" | jq -r '.errors[0]' 2>/dev/null || echo "$TOKEN_RESPONSE")"
            echo "::error::Pre-flight failed: Vault token invalid. Run: AGENTS.md → Vault Token 过期"
            exit 1
          fi
          
          echo "✅ Vault token is valid"

      - name: Apply Infrastructure (L2 - Platform)
        working-directory: 2.platform
        env:
          TF_LOG: INFO
        run: |
          # Ensure platform workspace is default
          terraform workspace select default || terraform workspace new default
          terraform apply -auto-approve || {
             echo "::error::L2 Platform Apply failed."
             exit 1
          }

      - name: Verify L2 (Vault Status)
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          echo "Checking Vault Status..."
          kubectl -n platform get pods -l app.kubernetes.io/name=vault
          # Simple check if Vault is running and accessible inside cluster
          # (Detailed check requires init, which is handled by raft join in L2 setup)

      # =========================================================
      # 3. Data Layer (L3): Databases
      # =========================================================
      - name: Setup Terraform Environment (L3)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "3.data"
          terraform_version: 1.6.6
          tf_state_key: "k3s/data-prod.tfstate" # Hardcoded for main branch -> prod
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}
          vault_address: https://secrets.${{ secrets.INTERNAL_DOMAIN }}

      - name: Pre-flight Image Check (L3)
        if: github.event_name == 'push'
        run: |
          python3 0.tools/check_images.py \
            "bitnamilegacy/postgresql:17.2.0-debian-12-r8" \
            "bitnamilegacy/redis:7.4.2-debian-12-r0" \
            "bitnamilegacy/clickhouse:25.7.5-debian-12-r0" \
            "bitnamilegacy/clickhouse-keeper:25.7.5-debian-12-r0" \
            "arangodb/arangodb:3.11.8"

      - name: Apply Infrastructure (L3 - Data)
        if: github.event_name == 'push'
        working-directory: 3.data
        env:
          TF_LOG: INFO
        run: |
          # Use workspace based on branch mapping (main -> prod)
          terraform workspace select prod || terraform workspace new prod
          terraform apply -auto-approve || {
             echo "::error::L3 Data Apply failed."
             exit 1
          }

      - name: Verify Data Services Health (L3)
        if: github.event_name == 'push'
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          NS="data-prod" # Mapping main branch to prod env
          TIMEOUT=180
          
          echo "=== 1. Waiting for Pod Readiness (K8s Layer) ==="
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=postgresql --timeout=${TIMEOUT}s || echo "⚠️ PostgreSQL wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=redis --timeout=${TIMEOUT}s || echo "⚠️ Redis wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=clickhouse --timeout=${TIMEOUT}s || echo "⚠️ ClickHouse wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l arango_deployment=arangodb --timeout=${TIMEOUT}s || echo "⚠️ ArangoDB wait timeout"
          
          echo ""
          
          echo "=== 2. Application Layer Connectivity Checks ==="
          
          # Retrieve Secrets for Health Checks
          REDIS_PASS=$(kubectl get secret -n $NS redis -o jsonpath="{.data.redis-password}" | base64 -d)
          CH_PASS=$(kubectl get secret -n $NS clickhouse -o jsonpath="{.data.admin-password}" | base64 -d)

          echo "Checking PostgreSQL..."
          kubectl exec -n $NS postgresql-0 -- pg_isready -U postgres && echo "✅ PostgreSQL: Ready" || echo "❌ PostgreSQL: Not Ready"
          
          echo "Checking Redis..."
          kubectl exec -n $NS redis-master-0 -- redis-cli -a "$REDIS_PASS" ping && echo "✅ Redis: PONG" || echo "❌ Redis: Not Ready"
          
          echo "Checking ClickHouse..."
          kubectl exec -n $NS clickhouse-shard0-0 -- clickhouse-client --user default --password "$CH_PASS" --query "SELECT 1" && echo "✅ ClickHouse: Ready" || echo "❌ ClickHouse: Not Ready"
          
          echo "Checking Keeper..."
          # Use clickhouse-keeper CLI instead of nc (not available in container)
          kubectl exec -n $NS clickhouse-keeper-0 -- clickhouse-keeper-client -q "stat" 2>/dev/null && echo "✅ Keeper: Ready" || echo "❌ Keeper: Not Ready (non-blocking)"
          
          echo "Checking ArangoDB..."
          # Get the actual ArangoDB pod name (dynamic name pattern)
          ARANGO_POD=$(kubectl get pods -n $NS -l arango_deployment=arangodb -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
          if [ -n "$ARANGO_POD" ]; then
            kubectl exec -n $NS "$ARANGO_POD" -- curl -sf http://localhost:8529/_api/version && echo "✅ ArangoDB: Ready" || echo "❌ ArangoDB: Not Ready"
          else
            echo "❌ ArangoDB: Pod not found"
          fi
          
          echo ""
          echo "=== Summary ==="
          kubectl get pods -n $NS -o wide

      # =========================================================
      # 4. App Layer (L4): Applications
      # =========================================================
      - name: Setup Terraform Environment (L4)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "4.apps"
          terraform_version: 1.6.6
          tf_state_key: "k3s/apps-prod.tfstate" # Hardcoded for main branch -> prod
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}
          vault_address: https://secrets.${{ secrets.INTERNAL_DOMAIN }}

      - name: Apply Infrastructure (L4 - Apps)
        if: github.event_name == 'push'
        working-directory: 4.apps
        env:
          TF_LOG: INFO
          TF_VAR_environment: prod
        run: |
          # Skip if no .tf files present (L4 is placeholder until apps are added)
          if ! ls *.tf 1> /dev/null 2>&1; then
            echo "⏭️ Skipping L4 Apply: No .tf files in 4.apps/"
            exit 0
          fi
          
          terraform workspace select prod || terraform workspace new prod
          terraform apply -auto-approve || {
             echo "::error::L4 Apps Apply failed."
             exit 1
          }

      - name: Verify L4 (Endpoint Check)
        if: github.event_name == 'push'
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          # Skip if L4 was skipped (no .tf files)
          if [ ! -f "4.apps/*.tf" ] 2>/dev/null && ! ls 4.apps/*.tf 1>/dev/null 2>&1; then
            echo "⏭️ Skipping L4 Verify: No apps deployed"
            exit 0
          fi
          
          echo "Verifying L4 Applications..."
          kubectl get pods -n apps-prod || true

      - name: Upload kubeconfig
        uses: actions/upload-artifact@v4
        with:
          name: kubeconfig
          path: ${{ env.KUBECONFIG_PATH }}
