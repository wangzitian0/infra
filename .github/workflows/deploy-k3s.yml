name: Deploy k3s to VPS

on:
  push:
    branches: [main]
    paths:
      - "1.bootstrap/**"
      - ".github/workflows/deploy-k3s.yml"
      - ".github/actions/**"
  workflow_dispatch: {}

env:
  TF_IN_AUTOMATION: "true"
  TF_INPUT: "false"

jobs:
  apply:
    runs-on: ubuntu-latest

    permissions:
      contents: read

    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Setup Terraform Environment (L1)
        uses: ./.github/actions/terraform-setup
        with:
          working_directory: "1.bootstrap"
          terraform_version: 1.6.6
          tf_state_key: "k3s/terraform.tfstate"
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          r2_bucket: ${{ secrets.R2_BUCKET }}
          r2_account_id: ${{ secrets.R2_ACCOUNT_ID }}
          vps_host: ${{ secrets.VPS_HOST }}
          vps_ssh_key: ${{ secrets.VPS_SSH_KEY }}
          vps_user: ${{ secrets.VPS_USER }}
          vps_ssh_port: ${{ secrets.VPS_SSH_PORT }}
          k3s_cluster_name: ${{ secrets.K3S_CLUSTER_NAME }}
          k3s_api_endpoint: ${{ secrets.K3S_API_ENDPOINT }}
          k3s_channel: ${{ secrets.K3S_CHANNEL }}
          k3s_version: ${{ secrets.K3S_VERSION }}
          vault_postgres_password: ${{ secrets.VAULT_POSTGRES_PASSWORD }}
          github_token: ${{ secrets.GH_PAT || github.token }}
          atlantis_webhook_secret: ${{ secrets.ATLANTIS_WEBHOOK_SECRET }}
          cloudflare_api_token: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          cloudflare_zone_id: ${{ secrets.CLOUDFLARE_ZONE_ID }}
          base_domain: ${{ secrets.BASE_DOMAIN }}
          internal_domain: ${{ secrets.INTERNAL_DOMAIN }}
          internal_zone_id: ${{ secrets.INTERNAL_ZONE_ID }}
          github_app_id: ${{ secrets.ATLANTIS_GH_APP_ID }}
          github_app_key: ${{ secrets.ATLANTIS_GH_APP_KEY }}
          atlantis_web_password: ${{ secrets.ATLANTIS_WEB_PASSWORD }}
          # OAuth2-Proxy (GitHub OAuth for Dashboard protection)
          github_oauth_client_id: ${{ secrets.GH_OAUTH_CLIENT_ID }}
          github_oauth_client_secret: ${{ secrets.GH_OAUTH_CLIENT_SECRET }}
          # L3: Vault Access
          vault_root_token: ${{ secrets.VAULT_ROOT_TOKEN }}

      # Pre-flight check: Validate Helm URLs and other common issues
      - name: Pre-flight Check
        run: |
          chmod +x 0.tools/preflight-check.sh
          0.tools/preflight-check.sh

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: v1.28.4

      # Import existing k8s resources that Terraform wants to manage
      # This prevents "already exists" errors when adopting existing resources
      - name: Import Existing Resources
        working-directory: 1.bootstrap
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          # Import local-path-config ConfigMap if not in state
          # Note: Using flat resource path after directory restructure
          if ! terraform state show kubernetes_config_map_v1.local_path_config 2>/dev/null; then
            echo "Importing local-path-config ConfigMap..."
            terraform import kubernetes_config_map_v1.local_path_config kube-system/local-path-config || true
          fi
          
          # If Atlantis already exists in cluster but was lost from state, import it.
          # Never remove state or delete in-cluster resources automatically from CI.
          if terraform state show helm_release.atlantis 2>/dev/null; then
            echo "Atlantis helm_release already tracked in Terraform state."
          else
            echo "Attempting to import existing Atlantis helm release (if present)..."
            terraform import helm_release.atlantis bootstrap/atlantis || true
          fi
          
          # Force clean Helm secrets to prevent "cannot re-use a name" error
          # This handles the case where Terraform state is clean but Cluster has lingering Helm release
          echo "Force cleaning lingering Helm secrets..."
          echo "Force cleaning lingering Helm secrets..."
          # Use pattern matching instead of labels to avoid accidents and ensure we hit the right target
          kubectl -n platform get secrets -o name | grep "sh.helm.release.v1.atlantis" | xargs -r kubectl -n platform delete || true
          kubectl delete deployment -n platform atlantis || true
          kubectl delete svc -n platform atlantis || true
          kubectl delete statefulset -n platform atlantis || true

      # Apply all infrastructure (moved blocks require full apply, not targeted)
      # Note: -target=module.nodep fails when moved.tf has pending resource moves
      # Pre-flight Image Safety Check (L3)
      # Prevents deploying with missing/invalid images which causes CrashLoopBackOff
      - name: Pre-flight Image Check (L3)
        if: github.event_name == 'push'
        run: |
          python3 tools/check_images.py \
            "bitnamilegacy/postgresql:17.2.0-debian-12-r8" \
            "bitnamilegacy/redis:7.4.2-debian-12-r0" \
            "bitnamilegacy/clickhouse:25.7.5-debian-12-r0" \
            "bitnamilegacy/clickhouse-keeper:25.7.5-debian-12-r0" \
            "arangodb/arangodb:3.11.8"

      - name: Apply Infrastructure (L3 - Data)
        if: github.event_name == 'push'
        working-directory: 3.data
        env:
          TF_LOG: INFO
        run: |
          # Use workspace based on branch mapping (main -> prod)
          terraform workspace select prod || terraform workspace new prod
          terraform apply -auto-approve || {
             echo "::error::L3 Data Apply failed."
             exit 1
          }

      - name: Apply Infrastructure (L1)
        working-directory: 1.bootstrap
        env:
          TF_LOG: INFO
        run: |
          terraform apply -auto-approve || {
            echo "::error::L1 Apply failed. Checking pod status..."
            kubectl get pods -A -o wide 2>/dev/null || true
            kubectl describe pods -n platform 2>/dev/null | tail -50 || true
            exit 1
          }

      # Deep Health Check: Verify not just "Running", but "Service Ready"
      - name: Verify Data Services Health
        if: github.event_name == 'push'
        env:
          KUBECONFIG: ${{ env.KUBECONFIG_PATH }}
        run: |
          NS="data-prod" # Mapping main branch to prod env
          TIMEOUT=180    # Increased timeout for initial pull/startup
          
          echo "=== 1. Waiting for Pod Readiness (K8s Layer) ==="
          # 'Ready' condition means Liveness/Readiness probes defined in Helm passed
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=postgresql --timeout=${TIMEOUT}s || echo "⚠️ PostgreSQL wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=redis --timeout=${TIMEOUT}s || echo "⚠️ Redis wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=clickhouse --timeout=${TIMEOUT}s || echo "⚠️ ClickHouse wait timeout"
          kubectl wait --for=condition=Ready pod -n $NS -l app.kubernetes.io/name=arangodb --timeout=${TIMEOUT}s || echo "⚠️ ArangoDB wait timeout"
          
          echo ""
          echo "=== 2. Application Layer Connectivity Checks ==="
          
          echo "Checking PostgreSQL..."
          kubectl exec -n $NS postgresql-0 -- pg_isready -U postgres && echo "✅ PostgreSQL: Ready to accept connections" || echo "❌ PostgreSQL: Not Ready"
          
          echo "Checking Redis..."
          kubectl exec -n $NS redis-master-0 -- redis-cli ping && echo "✅ Redis: PONG (Ready)" || echo "❌ Redis: Not Ready"
          
          echo "Checking ClickHouse..."
          # Select 1 confirms SQL interface is up
          kubectl exec -n $NS clickhouse-shard0-0 -- clickhouse-client --query "SELECT 1" && echo "✅ ClickHouse: SQL Ready" || echo "❌ ClickHouse: Not Ready"
          
          echo "Checking ClickHouse Keeper..."
          # 'ruok' is the ZooKeeper 4-letter word for health
          kubectl exec -n $NS clickhouse-keeper-0 -- bash -c 'echo ruok | nc localhost 9181' && echo "✅ Keeper: imok (Ready)" || echo "❌ Keeper: Not Ready"
          
          echo "Checking ArangoDB..."
          # Check API version endpoint
          kubectl exec -n $NS -l arango_deployment=arangodb -- curl -sf http://localhost:8529/_api/version && echo "✅ ArangoDB: API Ready" || echo "❌ ArangoDB: Not Ready"
          
          echo ""
          echo "=== Summary ==="
          kubectl get pods -n $NS -o wide

      - name: Upload kubeconfig
        uses: actions/upload-artifact@v4
        with:
          name: kubeconfig
          path: ${{ env.KUBECONFIG_PATH }}
