name: "Terraform Setup"
description: "Sets up Terraform, loads secrets via Python, configures SSH, and fetches kubeconfig"
inputs:
  terraform_version:
    description: "Terraform version to use"
    required: true
    default: "1.6.6"
  secrets_json:
    description: "All GitHub Secrets in JSON format (from toJSON(secrets))"
    required: true
  tf_state_key:
    description: "Terraform state file key in R2 bucket"
    required: false
    default: "terraform.tfstate"
  working_directory:
    description: "Working directory for Terraform operations"
    required: false
    default: "1.bootstrap"

runs:
  using: "composite"
  steps:
    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ inputs.terraform_version }}
        terraform_wrapper: false

    - name: Load and Map Secrets (Python)
      shell: bash
      env:
        INPUT_SECRETS_JSON: ${{ inputs.secrets_json }}
      run: |
        python3 0.tools/ci_load_secrets.py

    - name: Debug Environment
      shell: bash
      run: |
        echo "=== DEBUG: Checking Environment Variables ==="
        env | grep -E "AWS_|R2_|VPS_|TF_VAR_|GITHUB_" | sort || true
        echo "=== DEBUG: End ==="

    - name: Setup SSH and Directory
      shell: bash
      run: |
        set -euo pipefail
        
        # VPS_SSH_KEY was exported to GITHUB_ENV by Python script
        # So it is available as $VPS_SSH_KEY in the shell
        
        SSH_PORT_VALUE="${VPS_SSH_PORT:-22}"
        
        WORKING_DIR="${{ inputs.working_directory }}"
        mkdir -p "${WORKING_DIR}"

        # Setup SSH for fetching kubeconfig
        mkdir -p ~/.ssh
        echo "${VPS_SSH_KEY}" > ~/.ssh/id_rsa
        chmod 600 ~/.ssh/id_rsa
        ssh-keyscan -p "${SSH_PORT_VALUE}" "${VPS_HOST}" >> ~/.ssh/known_hosts

    - name: Terraform init
      working-directory: ${{ inputs.working_directory }}
      shell: bash
      env:
        # We ONLY map inputs here. 
        # Secrets (AWS_*, R2_*) are already in the global env from Python script.
        # Mapping them as ${{ env.VAR }} here would overwrite them with empty strings!
        STATE_KEY: ${{ inputs.tf_state_key }}
      run: |
        echo "DEBUG: STATE_KEY is '${STATE_KEY}'"
        # Debug: Check if variables are present (without revealing values)
        if [ -n "$AWS_ACCESS_KEY_ID" ]; then echo "DEBUG: AWS_ACCESS_KEY_ID is set"; else echo "ERROR: AWS_ACCESS_KEY_ID is missing"; fi
        
        terraform init -input=false \
          -backend-config="bucket=${R2_BUCKET}" \
          -backend-config="key=${STATE_KEY}" \
          -backend-config="endpoints={s3=\"https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com\"}"

    # Fetch kubeconfig explicity (before Plan, so state refresh works)
    - name: Fetch kubeconfig
      shell: bash
      run: |
        set -euo pipefail
        SSH_PORT="${VPS_SSH_PORT:-22}"
        # K3S_CLUSTER_NAME is set by Python script if missing
        CLUSTER_NAME_VALUE="${K3S_CLUSTER_NAME}"

        KUBECONFIG_PATH="${GITHUB_WORKSPACE}/1.bootstrap/output/${CLUSTER_NAME_VALUE}-kubeconfig.yaml"
        mkdir -p "$(dirname "${KUBECONFIG_PATH}")"
        
        echo "Attempting to fetch kubeconfig from VPS..."
        if ssh -i ~/.ssh/id_rsa -p "${SSH_PORT}" -o StrictHostKeyChecking=no "root@${VPS_HOST}" "sudo cat /etc/rancher/k3s/k3s.yaml" > "${KUBECONFIG_PATH}"; then
          echo "Kubeconfig fetched successfully."
          # Replace localhost with VPS Host
          sed -i "s/127.0.0.1/${VPS_HOST}/g" "${KUBECONFIG_PATH}"
          chmod 600 "${KUBECONFIG_PATH}"
          echo "KUBECONFIG_PATH=${KUBECONFIG_PATH}" >> "${GITHUB_ENV}"
          echo "TF_VAR_kubeconfig_path=${KUBECONFIG_PATH}" >> "${GITHUB_ENV}"
        else
          echo "Warning: Failed to fetch kubeconfig. This is expected if the cluster is not yet deployed."
          rm -f "${KUBECONFIG_PATH}"
        fi
